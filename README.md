# Concurrent Containers and Condition Variable Wrapper – WRITEUP

## 1. Project Overview

In this project I implemented and compared several concurrent stack and queue algorithms, and I also implemented a wrapper around `std::condition_variable` that hides spurious wakeups from user code. The goal of the project is to study how different synchronization strategies behave under varying thread counts and workloads, and to support the analysis with measurements from `perf stat -d`. All experiments and implementations target a Linux environment with C++20.

For stacks, I implemented four variants: a single-global-lock (SGL) stack, a Treiber lock-free stack, an elimination stack built on top of a Treiber-style core, and a flat-combining stack. For queues, I implemented an SGL queue, a Michael–Scott (MS) lock-free queue, and a flat-combining queue. Each data structure exposes a simple `push`/`pop` or `enqueue`/`dequeue` interface that is used by a shared benchmarking harness. The harness runs uniform workloads with a fixed total number of operations and variable thread counts, and it records latency and throughput for each implementation.

In addition to the containers, I implemented `CVNoSpurious`, a wrapper around `std::condition_variable` that is source-compatible at the basic interface level (`wait`, `wait(pred)`, `notify_one`, `notify_all`), but guarantees that the plain `wait(lock)` overload does not return due to spurious wakeups. Internally, this wrapper uses a generation counter and always calls the predicate form of `wait`, so that only real notifications advance the generation and allow a thread to proceed. I validated this wrapper with targeted tests that exercise both `notify_one` and `notify_all` under multiple waiting threads.

Overall, the project allows me to compare lock-based, lock-free, and flat-combining designs on the same hardware and under the same workload generator. The `run_one` binary provides simple single-implementation runs that integrate cleanly with `perf`, while the `bench` binary produces comparative runs and CSV-style summaries that are convenient for table generation in the write-up.

## 2. Code Organization

The repository is organized around three main directories: `include/`, `src/`, and `tests/`, plus a single top-level `Makefile`. All public container interfaces and support utilities live under `include/`. The `src/` directory contains the main benchmarking and driver programs. The `tests/` directory contains small focused tests that check functional correctness and specific behavioral properties, including the condition-variable wrapper.

In `include/`, the stack implementations are split into separate headers: `sgl_stack.h` defines `SGLStack<T>` backed by a standard container and protected by a single `std::mutex`; `treiber_stack.h` defines `TreiberStack<T>` using a singly linked list of heap nodes and an atomic top pointer; `elimination_stack.h` defines `EliminationStack<T>`, which builds on a Treiber-style core and adds an elimination array where opposing operations can meet; `flat_combining_stack.h` defines `FlatCombiningStack<T>`, which uses per-thread request records and a combiner thread that applies operations to an underlying stack. Queue implementations follow the same pattern: `sgl_queue.h` defines `SGLQueue<T>` with a mutex and contiguous storage; `ms_queue.h` defines `MSQueue<T>` using the Michael–Scott algorithm with a dummy node, atomic head and tail pointers, and safe reclamation; `flat_combining_queue.h` defines `FlatCombiningQueue<T>` with per-thread requests and a central combiner. Common utilities such as small assertion helpers and shared `#include`s are in `common.h`, and the condition-variable wrapper is defined in `cv_nospurious.h` as `CVNoSpurious`.

The `src/` directory contains two main programs. `main_bench.cpp` builds the `bench` binary, which runs all stacks and queues for a given workload configuration and prints human-readable summaries and a CSV-style summary line per structure. `main_single.cpp` builds the `run_one` binary, which runs exactly one stack or queue implementation per invocation. `run_one` accepts command-line options `--kind` (stack or queue), `--algo` (sgl, treiber, elim, fc, ms), `--threads`, and `--ops`, and then executes a fixed pattern of pushes or enqueues followed by a drain. This design keeps the driver for perf runs small and predictable.

The `tests/` directory contains several small executables built via `make test TEST=name`. `test_stacks.cpp` exercises all stack implementations with single-threaded and small multi-threaded correctness checks, and verifies that pushes and pops match. `test_queues.cpp` performs similar checks for the queue implementations. `test_cv.cpp` focuses on `CVNoSpurious` and runs two scenarios: a single waiting thread with repeated `notify_one` calls, and multiple waiting threads with repeated `notify_all` calls, verifying that each notification leads to exactly one wake per waiting iteration. The Makefile supports building each test individually and building the two main benchmarking binaries.

## 3. Experimental Methodology

For all quantitative results in this project I followed one consistent methodology. I built the project with `g++` using C++20, optimization level `-O3`, POSIX threads, and architecture-specific tuning. The exact compile flags in the Makefile are:


std=gnu++20 -O3 -Wall -Wextra -pthread -march=native
This keeps the code portable while still letting the compiler optimize aggressively for the target machine.

I separated two kinds of executables. The bench binary (from main_bench.cpp) runs a whole suite of stacks and queues in one invocation and prints human-readable summaries and CSV-like lines for quick comparison. The run_one binary (from main_single.cpp) runs a single data structure per invocation and takes command-line arguments of the form:
./build/run_one --kind=stack --algo=treiber --threads=16 --ops=10000
./build/run_one --kind=queue --algo=ms --threads=16 --ops=10000
The --kind argument selects between stack and queue. The --algo argument selects one of sgl, treiber, elim, or fc for stacks, and one of sgl, ms, or fc for queues. The --threads argument controls the number of worker threads for stacks, and the number of producer threads for queues. The --ops argument specifies the total number of logical operations for the run. For stacks, this total is split evenly across threads as pushes, and then the benchmark drains the stack with pops in one thread to count how many items were actually produced. For queues, this total is split across producer threads as enqueues, and one consumer thread repeatedly dequeues until all items have been consumed.

For perf analysis I used only the run_one binary to keep each measurement tied to exactly one data structure and configuration. Each command uses perf stat -d to capture timing and selected microarchitectural statistics. A typical invocation looks like:
perf stat -d ./build/run_one --kind=stack --algo=treiber --threads=16 --ops=10000
The key metrics I used in the write-up are wall-clock time, total instructions, L1 data cache loads, and L1 data cache load misses. From these I computed throughput in Mops/s (millions of logical operations per second), instructions per operation, L1-misses per operation, and an L1 miss rate. All the numbers in the summary table come from repeated runs with ops=10000 and thread counts equal to 1 and 16. For each row in the table I ensured that the reported numbers match a concrete perf command with a fixed kind, algo, threads, and ops configuration.

## 4. Perf Results for Stacks

The table below contains all implementations, but in this section I focus on the stack rows. Each stack run performs 10000 logical pushes in total across all threads, and then one thread drains the stack with pops. Time is the end-to-end wall time for the push phase and the drain. Throughput is computed as `ops / time / 1e6`. Instructions per operation, L1-misses per operation, and L1 miss rate come from `perf stat -d`.

| Impl         | Kind  | Threads | Time (s) | Throughput (Mops/s) | Instr/op | L1-miss/op | L1 miss rate (%) |
|--------------|-------|---------|---------:|---------------------:|---------:|-----------:|------------------:|
| SGLStack     | stack | 1       | 0.0053  | 1.87                 | 510.0    | 9.61       | 7.23             |
| TREIBERStack | stack | 1       | 0.0049  | 2.03                 | 849.2    | 13.13      | 6.16             |
| ELIMStack    | stack | 1       | 0.0047  | 2.11                 | 776.5    | 11.83      | 6.24             |
| FCStack      | stack | 1       | 0.0037  | 2.73                 | 565.3    | 8.98       | 5.96             |
| SGLStack     | stack | 16      | 0.0069  | 1.45                 | 728.8    | 21.78      | 11.50            |
| TREIBERStack | stack | 16      | 0.0065  | 1.54                 | 1014.1   | 23.47      | 9.13             |
| ELIMStack    | stack | 16      | 0.0066  | 1.50                 | 957.1    | 22.63      | 9.54             |
| FCStack      | stack | 16      | 0.0054  | 1.85                 | 921.2    | 19.52      | 7.54             |

For one thread, FCStack reaches 2.73 Mops/s with 565.3 instructions per operation and 8.98 L1-misses per operation. SGLStack reaches 1.87 Mops/s with 510 instructions per operation and 9.61 L1-misses per operation. TreiberStack and ELIMStack reach 2.03 and 2.11 Mops/s with 849.2 and 776.5 instructions per operation. I read these numbers together with the implementation structure. SGLStack holds one `std::mutex` and modifies a contiguous container in each push and pop, so each operation executes a lock, a small amount of container work, and an unlock. TreiberStack and ELIMStack operate on heap nodes behind an atomic top pointer and rely on CAS. Each push and pop touches pointer fields and participates in a CAS loop on `std::atomic<Node*>`. That pattern explains the higher instruction count and L1-misses per operation for TreiberStack and ELIMStack. ELIMStack uses an elimination array, so some push/pop pairs complete by exchanging values in the array and never touch the top pointer, which explains why ELIMStack sits between SGLStack and TreiberStack in instructions per operation. FCStack places requests in per-thread slots and then runs a combiner that applies these requests to the underlying stack. With one thread, the combiner path collapses into a simple sequence of stack operations, which matches the higher throughput and moderate instruction count.

At sixteen threads, shared locations in each stack become a point of contention. SGLStack now runs at 1.45 Mops/s with 728.8 instructions per operation and 21.78 L1-misses per operation. Every push and pop acquires the same mutex, and different cores enter and leave the critical section. The protected data moves across cores, and the lock variable produces extra coherence traffic, which shows up as increased instructions and L1 activity per operation. TreiberStack runs at 1.54 Mops/s with 1014.1 instructions per operation and 23.47 L1-misses per operation. ELIMStack runs at 1.50 Mops/s with 957.1 instructions per operation and 22.63 L1-misses per operation. In both stacks, many threads attempt CAS on the same top pointer; failures in the CAS loops and extra hazard-pointer or reclamation bookkeeping increase both the static instruction count and the number of data cache loads. The elimination array in ELIMStack removes some CAS work but introduces array slot probes and extra atomics.

FCStack handles sixteen threads with a different pattern. Threads write their operations into dedicated request records, and one combiner thread reads these records and applies the operations to the shared stack. FCStack reaches 1.85 Mops/s with 921.2 instructions per operation and 19.52 L1-misses per operation. The combiner loop introduces extra instructions for scanning request slots and coordinating ownership of the combiner role, and this lifts Instr/op compared to the one-thread case. At the same time, only the combiner mutates the core stack structure, so the top pointer and container body do not carry the same level of CAS or mutex contention that appears in the other implementations. For this workload and thread count, the numbers indicate that flat combining establishes a different concurrency tradeoff: more work inside the combiner per operation, less contention on the main data structure, and a resulting throughput higher than the other stacks in the table at sixteen threads.

## 5. Perf Results for Queues

The queue rows in the same table use 10000 total logical enqueues per run. Producer threads cooperate to enqueue the total count, and one consumer thread drains the queue until all enqueued items have been removed. The same `perf stat -d` metrics are used, and throughput is again computed as `ops / time / 1e6`.

| Impl         | Kind  | Threads | Time (s) | Throughput (Mops/s) | Instr/op | L1-miss/op | L1 miss rate (%) |
|--------------|-------|---------|---------:|---------------------:|---------:|-----------:|------------------:|
| SGLQueue     | queue | 1       | 0.0044  | 2.29                 | 540.8    | 10.57      | 7.39             |
| MSQueue      | queue | 1       | 0.0064  | 1.57                 | 785.7    | 23.96      | 11.52            |
| FCQueue      | queue | 1       | 0.0045  | 2.24                 | 645.9    | 11.49      | 6.76             |
| SGLQueue     | queue | 16      | 0.0054  | 1.87                 | 685.3    | 20.21      | 11.19            |
| MSQueue      | queue | 16      | 0.0071  | 1.41                 | 1096.8   | 28.11      | 10.05            |
| FCQueue      | queue | 16      | 0.0054  | 1.84                 | 944.3    | 18.87      | 7.12             |

With one thread, SGLQueue reaches 2.29 Mops/s with 540.8 instructions per operation and 10.57 L1-misses per operation. FCQueue reaches 2.24 Mops/s with 645.9 instructions per operation and 11.49 L1-misses per operation. MSQueue reaches 1.57 Mops/s with 785.7 instructions per operation and 23.96 L1-misses per operation. SGLQueue holds a mutex around a queue backed by contiguous storage, so each enqueue and dequeue performs a lock, updates head or tail indices, accesses a compact region in memory, and then releases the lock. FCQueue for one thread runs through its combiner path with a trivial pattern because there is only one producer; the queue updates and combiner logic run on the same core and the queue core sees one logical producer and consumer. MSQueue follows the Michael–Scott linked-list queue design with one dummy node. Each enqueue uses atomic operations on the tail pointer and node fields, and each dequeue uses atomic operations on the head pointer and node fields. Node storage spreads across the heap, and with reclamation logic each operation touches more locations and metadata, which matches the higher values for instructions and L1 misses per operation.

At sixteen threads, contention patterns change. SGLQueue now runs at 1.87 Mops/s with 685.3 instructions per operation and 20.21 L1-misses per operation. The single mutex protects the enqueue and dequeue operations, so producers and the consumer wait for access to the critical section. This serialization pushes time up relative to the one-thread run, and the lock variable and queue buffer move across cores as ownership of the lock passes between threads. MSQueue runs at 1.41 Mops/s with 1096.8 instructions per operation and 28.11 L1-misses per operation. These numbers align with the structure of the algorithm. Many producer threads operate on the same head and tail fields and the same next pointers on the linked list. Each race on these pointers yields extra CAS attempts, and reclaiming nodes demands further hazard-pointer or epoch bookkeeping. The linked layout of nodes across the heap leads to many data cache loads and misses, which matches the L1-miss/op and miss-rate numbers.

FCQueue at sixteen threads reaches 1.84 Mops/s with 944.3 instructions per operation and 18.87 L1-misses per operation. FCQueue accepts operations in per-thread request records, and a combiner dequeues and enqueues on the shared queue. Producers write to their own slots and either become the combiner or wait for the current combiner to flush the pending requests. This path explains the instruction count: each logical enqueue involves both the request write and the combiner’s scan of request slots and application of operations to the queue. At the same time, the internal queue does not experience simultaneous CAS or lock operations from many producers, because only the combiner writes its head and tail indices. The L1-miss/op value sits between SGLQueue and MSQueue at sixteen threads, and the throughput numbers for SGLQueue and FCQueue at sixteen threads are close while both stay above the MSQueue throughput for the same configuration. I read these results as evidence that, for this queue workload, both a simple global lock and flat combining handle moderate levels of concurrency better than a fully lock-free linked queue with reclamation, at least in terms of operations per second at 10000 operations and the chosen thread counts.

## 6. Condition Variable Wrapper (CVNoSpurious)

The project includes a wrapper around `std::condition_variable` named `CVNoSpurious`. The purpose of this wrapper is to preserve the standard interface of a condition variable while guaranteeing that the plain `wait(lock)` overload never returns due to a spurious wakeup. Ordinary `std::condition_variable` requires user code to guard `wait` with a predicate or an explicit loop; `CVNoSpurious` moves that responsibility inside the wrapper.

The design of `CVNoSpurious` adds one state variable on top of the underlying condition variable: an atomic generation counter named `seq_`. The class stores an instance of `std::condition_variable` and this atomic `std::size_t`. The methods `notify_one()` and `notify_all()` both increment `seq_` with `fetch_add(1, std::memory_order_release)` and then call the corresponding method on the underlying condition variable. Each call to `wait(std::unique_lock<std::mutex>& lock)` captures the current value of `seq_` in a local variable `my_seq`. The wrapper then invokes `cv_.wait(lock, predicate)` on the underlying condition variable, where the predicate reads `seq_` and returns `true` once `seq_ != my_seq`. As a result, plain `wait(lock)` in user code is equivalent to “wait until some notification advances the generation counter,” and any wakeup that does not accompany an increment of `seq_` fails the predicate and remains hidden. The template overload `wait(lock, pred)` simply forwards to `cv_.wait(lock, pred)` to preserve the standard semantics of predicate-based waiting.

This design respects the usual memory ordering expectations for condition variables. Writers call `notify_one` or `notify_all` while holding the same mutex that protects the shared state. The mutex release synchronizes the writes to the shared state with the generation increment and the notification. On the waiting side, the `wait` call releases and reacquires the same mutex, and the predicate observes both the updated shared state and the updated `seq_`. Broadcast behavior also follows the usual contract. A `notify_all()` increments the generation once, and all threads currently waiting on `wait(lock)` see `seq_` change from their recorded `my_seq` to the new value. The predicate evaluates to `true` for all of them, so they all return from `wait(lock)`. If multiple rounds of broadcast are required, user code calls `notify_all()` multiple times. Each call produces one increment of the generation, and the wrapper enforces that each loop iteration in a waiting thread corresponds to exactly one such increment.

The test file `tests/test_cv.cpp` validates two important properties of `CVNoSpurious`. The first test, `test_single_waiter`, starts one worker thread that holds a `std::unique_lock<std::mutex>` and calls `cv.wait(lock)` in a loop for a small number of rounds (for example 10). The main thread, after confirming that the worker is ready, performs the same number of `notify_one()` calls on the wrapper. After all notifications, the test checks that the worker’s wake counter equals the number of notifications. This scenario checks that there is a one-to-one relationship between `notify_one` calls and successful `wait` returns and that extra wakeups do not appear. The second test, `test_broadcast_many_waiters`, creates several worker threads. Each worker calls `cv.wait(lock)` in a loop for a fixed number of rounds, and the main thread performs that many `notify_all()` calls. Each worker counts its own wakes. After joining all workers, the test checks that each worker’s wake count matches the number of broadcasts. This scenario checks that `notify_all` reaches all current waiters and that the wrapper does not suppress legitimate wakeups.

In both tests the output shows the progress of notifications and wakes, which helps interpret and present behavior in the write-up and during grading. The tests intentionally call the non-predicate `wait(lock)` overload in all waiting loops so that the wrapper, rather than user code, handles spurious wakeups. This approach matches the assignment requirement that the wrapper should handle spurious wakeups internally and preserve a clean interface to callers.


## 7. File-by-File Description

The repository contains several source and header files that each serve one clear role. This section lists the main files and describes their responsibilities.

`Makefile` holds the build rules for the project. It defines the compiler, flags, include paths, and targets. The key targets are `all` (builds the main benchmark binary `bench`), `bench` (explicit alias for the same), `run_one` (builds the single-structure driver), `test` (builds a specific test given `TEST=name`), and `clean` (removes all files in the `build/` directory). The Makefile also creates the `build/` directory if it does not exist and tracks dependencies via generated `.d` files.

In the `include/` directory, each concurrent data structure lives in its own header. `sgl_stack.h` defines `SGLStack<T>`, a stack implemented with a standard sequential container and protected by one `std::mutex`. Methods `push` and `pop` acquire the mutex, perform the operation on the container, and release the mutex. `treiber_stack.h` defines `TreiberStack<T>`, a classic lock-free stack that uses a singly linked list and an `std::atomic<Node*>` for the top pointer. Pushes and pops use CAS on the top pointer to insert and remove nodes. `elimination_stack.h` defines `EliminationStack<T>`, which reuses a Treiber-style core and augments it with an elimination array that allows push and pop operations from different threads to rendezvous and exchange values without updating the central top pointer. `flat_combining_stack.h` defines `FlatCombiningStack<T>`, which uses per-thread request records and a combiner loop that scans requests and applies them to an underlying sequential stack representation.

`sgl_queue.h` defines `SGLQueue<T>`, a queue implemented on top of a contiguous container and protected by a single mutex. `ms_queue.h` defines `MSQueue<T>`, a Michael–Scott queue with a dummy node, atomic head and tail pointers, and logic to enqueue nodes at the tail and dequeue nodes by advancing the head. The implementation manages nodes on the heap and includes cleanup of nodes once the queue is drained. `flat_combining_queue.h` defines `FlatCombiningQueue<T>`, which uses the same per-thread request and combiner pattern as the flat-combining stack, but with enqueue and dequeue operations on a queue instead of a stack.

`common.h` collects shared includes and small utilities. It includes `<atomic>`, `<chrono>`, `<thread>`, `<mutex>`, `<condition_variable>`, and the headers for all stacks and queues. It defines a `check(bool, const char*)` helper that prints a diagnostic message and aborts when an assertion fails. This helper is used throughout tests and benchmarks to guard basic invariants such as “pushed count equals popped count.” `cv_nospurious.h` defines the `CVNoSpurious` wrapper described in the previous section.

In the `src/` directory, `main_bench.cpp` builds the `bench` executable. This file defines templated helper functions for stack and queue benchmarks. These helpers create an instance of a stack or queue, spawn the desired number of threads, perform a fixed number of operations per thread, join all threads, and record total time and counts. The program then prints a formatted section for stacks and a formatted section for queues, followed by a small CSV-style summary for all structures. `main_single.cpp` builds the `run_one` executable. It parses command-line arguments `--kind`, `--algo`, `--threads`, and `--ops`, selects the appropriate template instantiation (for example `TreiberStack<int>` or `MSQueue<int>`), and runs a single benchmark configuration, printing time, throughput, and sanity statistics. This program is the primary driver for `perf` runs.

The `tests/` directory contains small programs oriented around correctness and behavioral checks. `test_stacks.cpp` constructs each stack implementation and runs basic single-threaded tests that push and pop fixed sequences of integers, verifying that items are returned in last-in-first-out order and that counts match. It also runs a multi-threaded test where several threads push values into the stack and one thread pops until the stack is empty, then checks that all expected items were observed. `test_queues.cpp` performs similar tests for `SGLQueue`, `MSQueue`, and `FlatCombiningQueue`, using multiple producers and one consumer. `test_cv.cpp` contains the two tests described earlier for `CVNoSpurious`, and prints human-readable traces of notifications and wake events. The Makefile’s `test` target compiles each of these into a separate executable named `build/test_<name>`.

Together, these files form a structure where each concurrent data structure is isolated in its own header, the benchmarking logic lives in clear driver programs, and the tests provide targeted validation for stacks, queues, and the condition-variable wrapper.

## 8. Compilation Instructions

I compile the project with `g++` and `make` on Linux. The only required tools are a recent `g++` with C++20 support, `make`, and the standard C++ library with pthreads. The project does not depend on external libraries beyond the standard headers. For perf analysis I also install the `perf` tool from the Linux kernel tools package, but compilation does not require it.

The top-level `Makefile` controls the build. The main variables are:

make
CXX      := g++
CXXFLAGS := -std=gnu++20 -O3 -Wall -Wextra -pthread -march=native -MMD -MP
INCLUDES := -Iinclude
The -std=gnu++20 flag enables C++20, -O3 enables optimization, -pthread links against the POSIX threads library, and -march=native asks the compiler to tune and generate instructions for the current machine. The -MMD -MP flags generate dependency files so that incremental builds track header changes
Each test in the tests/ directory builds through the test target. To compile a single test, for example the condition-variable test, I run:
make test TEST=cv
This command compiles tests/test_cv.cpp and produces build/test_cv. The same pattern applies to other tests such as stacks, queues, or any other test source named test_*.cpp. For example:

make test TEST=stacks
make test TEST=queues


These commands produce build/test_stacks and build/test_queues respectively. All test binaries use the same compiler flags and include paths as the main benchmarks.
### Execution Instructions

./build/run_one --kind=<stack|queue> --algo=<name> --threads=N --ops=K

The --kind argument selects stack or queue mode. The --algo argument is one of:

sgl, treiber, elim, fc for stacks

sgl, ms, fc for queues

perf stat -d ./build/run_one --kind=stack --algo=fc --threads=1 --ops=10000

perf stat -d ./build/run_one --kind=queue --algo=sgl --threads=16 --ops=10000
## 9. Extant Bugs and Limitations

The current code base passes the functional tests for stacks, queues, and the condition-variable wrapper and produces consistent perf results for the configurations shown in the table. At the same time, there are several limitations and open points that I document here.

First, the elimination stack implementation follows the Treiber plus elimination-array pattern at a high level, but the elimination policy is tuned only for the small set of benchmarks in this project. The backoff and exchange timing in the elimination array is fixed and not adaptive. Under very high contention or very low contention, this policy may lead to either underuse or overuse of the elimination array, and the performance profile outside the tested ranges is not characterized in detail. The implementation focuses on correctness and reasonable behavior for the tested thread counts and operation counts rather than general-purpose tuning.

Second, the pair of flat-combining implementations (stack and queue) use a simple list of per-thread request records and a straightforward combiner election rule. In the current design, the list of requests is managed under a mutex when threads first register. There is no explicit deregistration path for threads that exit and later re-enter with a different identity. The benchmark harness uses a stable thread set for each run, so this limitation does not affect reported results. In a long-running application with threads that come and go many times, stale request nodes would accumulate, and the combiner scan cost would grow. The current design is therefore better viewed as a research prototype for static thread sets.

Third, the MSQueue implementation uses a basic reclamation scheme for nodes when the queue is drained. Lifetime management is safe for the benchmark patterns used in this project, where each run has a bounded set of enqueues followed by a drain and program exit. A complete production-quality MSQueue would integrate a more systematic reclamation strategy, such as hazard pointers or epoch-based reclamation, with careful consideration of long-running workloads. In this project, hazard-pointer style logic is present at a minimal level and is sufficient to prevent leaks under the tested patterns, but the implementation does not attempt to support arbitrary reuse of nodes across many benchmark phases within the same process.

Fourth, the benchmark harness itself uses a uniform workload pattern: each thread performs a fixed number of operations against one shared data structure. There is no mixed read/write scenario and no phase behavior. The numbers therefore characterize a narrow slice of possible behaviors. The results show how the implementations behave under this particular stress pattern, not under general mixed workloads with bursts, skewed access distributions, or key-based patterns.

Finally, the `CVNoSpurious` wrapper focuses strictly on the core `wait`/`wait(pred)` interface and the `notify_one` and `notify_all` calls. Other aspects of `std::condition_variable`, such as timed waits, are not wrapped. The design also relies on the conventional usage pattern where user code holds the associated mutex when calling `notify_one` or `notify_all`. The tests and the benchmark harness follow this pattern, but the wrapper does not attempt to detect or enforce it.

## 10. Discussion and Interpretation

The project brings together several classic concurrent data structure designs and one synchronization primitive wrapper and evaluates them under a controlled microbenchmark. The numbers and code structure illustrate multiple concurrency tradeoffs: simple coarse-grained locking versus fine-grained lock-free operations, centralized flat combining versus fully distributed progress, and in-structure synchronization versus external admission control through a combiner.

For the stacks, the comparison between SGLStack, TreiberStack, ELIMStack, and FCStack highlights different sources of cost. SGLStack pays once per operation for a global mutex and enjoys compact memory access, which works well at low concurrency and shifts toward lock convoy when thread count grows. TreiberStack and ELIMStack distribute progress with CAS-based operations on the top pointer. They avoid locks but pay for pointer chasing, CAS retries, and reclamation bookkeeping. ELIMStack adds an auxiliary structure to exchange opposite operations and relieve pressure on the top pointer, and the perf data reflects this change in the form of instruction counts that sit between the simple SGLStack and the more stressed TreiberStack. FCStack moves contention away from the core stack and into the combiner logic, establishing a pattern where many threads use a shared sequential core through a batched admission mechanism.

For the queues, the contrast between SGLQueue, MSQueue, and FCQueue shows similar themes. SGLQueue uses a plain mutex and a straightforward ring or deque layout, which performs well for moderate thread counts and a single consumer. MSQueue offers lock-free progress with two atomic pointers and a linked list of nodes. This structure increases robustness against blocking in the presence of misbehaving threads but introduces more scattered memory access and more complex progress logic. FCQueue again centralizes queue updates through a combiner while leaving per-thread request writes local. The queue experiments show that for the tested workload and thread counts the simple lock-based and flat-combining designs achieve higher throughput than the linked lock-free queue, while also using fewer instructions and fewer L1 cache misses per operation.

The condition-variable wrapper complements these data structures by addressing a classic practical issue in concurrent programming: spurious wakeups. Ordinary condition-variable usage in C++ requires every `wait` call to sit inside a loop that checks a predicate. `CVNoSpurious` embeds this pattern in the primitive itself for the plain `wait(lock)` overload by tracking a generation counter and always calling the predicate form of `wait`. The tests demonstrate that each `notify_one` causes exactly one waiting iteration to complete and that each `notify_all` causes every waiter to complete one iteration. This behavior simplifies reasoning about code that uses plain waits without losing compatibility with the standard predicate-based form.

## 11. Future Work

Several natural extensions to this project remain open. First, the hazard-pointer and reclamation infrastructure can be deepened and generalized. A more complete hazard-pointer library would manage protected pointers and retirement lists in a reusable component that both Treiber-based stacks and the MSQueue share. This change would make the memory-safety story clearer and would allow more aggressive experiments with higher operation counts and longer-running processes.

Second, the elimination stack and flat-combining structures can adopt adaptive policies. For the elimination stack, an adaptive backoff strategy that adjusts delay windows and slot selection based on recent success rates would likely change perf behavior under very high or very low contention. For flat combining, an adaptive combiner election and batching policy could switch between more centralized and more distributed modes as the thread count and operation mix change, reducing overhead when there are few threads and increasing batching when many threads are active.

Third, the benchmark harness can be expanded beyond uniform workloads. Scenarios with mixed operations (for example, different ratios of pushes and pops or enqueues and dequeues), key-range skew, or bursty arrival patterns would give a richer picture of each data structure’s strengths. The current harness already encodes the skeleton for such experiments; it needs more workload generators and summary code to support them.

Finally, the condition-variable wrapper can be extended to cover timed waits and integrated into higher-level synchronization abstractions. One direction is to define a small library of building blocks—bounded queues, barrier-like primitives, and work queues—that all use `CVNoSpurious` internally, and then measure both the functional simplicity and the perf cost relative to using raw `std::condition_variable` with manual predicate loops. This direction would bring the project closer to realistic concurrent systems code and allow a broader comparison between low-level primitives and composed abstractions.

